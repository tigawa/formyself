+++
title = "機械学習Bootcampに参加したときのメモ"
tags = ["python","pandas","機械学習"]
date = "2016-07-17"
banner = "banners/gear.jpg"
+++

2泊3日で機械学習を学ぶ合宿に参加してきました。
python、機械学習を全くしらないところから参加して
最後にはkaggleにチャレンジするところまで辿りつけました。

講師の先生の話が大変わかりやすかったお陰です。本当にありがとうございました。

ほとんど自分にしかわかりませんが、合宿中にメモしたことをUPします。

<!--more-->

- 講師：大塚誠
- 北海道釧路市出身

# 1日目

昼食のカレーが、服につくという失態をしてしまい。
どんよりした気持ちのまま合宿スタートです。

1日目は、scikit-learnの基本について学びました。

## 合宿のゴール
- scikit-learn(サイキットラーン)が使えるようになる。
  - scikit-learn = デファクトスタンダード

## scikit-learn
- Pythonの機械学習用ライブラリ

## 機械学習とは？

既存のデータの法則性を数理モデルに学習させ、
未知のデータに渡して学習結果を汎化させるための枠組み

## 機械学習の要素

- 既存のデータ
- 予測の良さを測る定規
- 学習アルゴリズム
- 予測モデル　←　これをつくることが目的
- 未知のデータ

## モデルとは
- モデル＝関数（数式）
- モデルの複雑さ
  - パラメータの複雑さ　＝　調整できるノブの数
    - パラメータ１つ
      - y = a0 = f(x:a0)
    - パラメータ２つ
      - y = a0 + a1X = f(x:a0,a1)
          　↑切片　↑傾き
    - パラメータ3つ
      - y=a0+a1x + a2x2=f(x:q0,q11,a2)

## モデルの種類
- 多項式モデル
  - パラメータは無限に増やせる
    - オーバフィッテイング
      複雑にフットさせすぎる

## バイアス・バリアンストレードオフ
- バイアスとバリアンスが交わるところを狙うべし
- バイアス(制約が強い)
  - パラメータを増やすと、バイアス（制約）はへる。
  - パラメータの数がおおいと、データの話を聞き過ぎる
- バリアンス（散らばり）

単純           | 複雑
:----------- | :----------
アンダーフィッティング | オーバーフィッティング


## 機械学習の３本柱

問題設定により、３つ別れれる。

### 教師あり学習（ほとんどがコレ）95%
- 分類
  ものが、売れるか？売れないか？　連続しないやつ
- 回帰
  出力が連続しており、グラフで表せる。

### 教師なし学習 4%
- 分布推定

  分布データの散らばりを予測する。
  取れた魚からサンプルがほしいのではなくてい、この近海の海のサンプルがほしい

- クラスタリング

  分布推定ができれば、クラスタリングはできる。
  山の数を数えればいいじゃん。

- 次元圧縮

  山の高さを示す。等高線ににている。（３次元になる）
  重さ・長さの２次元だったら、１次元にしてもいいじゃんって感じ

- 異常検知

### 強化学習 1%
- バンディット
- 強化学習
  コンピュータ動詞で戦わせて、学習させる。
- 逆強化学習

## scikit-learnのサイトトップページのサマリ
- 分類
- 回帰
- クラスタリング
- 次元圧縮
- モデル選択
- 前処理

## K分割交差検証
- 分割の目安
  - K=3 で広く浅くチェックして
  - K=5 そこそこ正確にしたい場合
  - K=10　ホントに正確にしたい場合

## 講師の先生のおすすめの資料
- [データサイエンティストのいろは](http://www.slideshare.net/hijiki_s/20150307-py-datatutorial)
  - シバタアキラさんの資料(p.21〜)
- [機械学習コンペションにおける予測モデル手法]((http://yukino.moo.jp/2016-07-06-DDBJ.pdf))

## 予測モデルの取出し方
modleをシリアライズして他でつかう。
modleの中に定規と学習アルゴリズム、予測モデルが入っている。

## pythonが人気の理由
- numpy があるからpythonは人気、行列がとくい！！
- しかも裏ではC言語で動いているので早い

## jupyter notebook のショートカット
- shift + tab
- shift + tab X 2 でメソッドの詳細を出してくれる

## マジックコマンド　jupyterに効くコード
`%matplotlib inline`
上記を書くと、'matplotlib.pyplot'で図が表示できる。


## plotの構成について
```python
fig,ax = plt.subplots(figsize=(6,6))
```

- fig(フィグ) = ウィンドウ
- ax(アクシス) = divみたいなやつ


```python
ax.scatter
```
- すかったー
２次元関数？

↓　axis 0
→  axis 1

```python
X_train, X_test, y_train, y_test = cross_validation.train_test_split(X,y,test_size=.2, random_state=42)
```

- random_state を指定すると常に同じ値がかえってくる。
- データはかならずシャッフルすべき
- random_state 42 の理由
  - 銀河ヒッチハイクガイド　からきている

## モデル選択
２つの意味がある

- モデルを選ぶ
  - Decision Tree
    - Random Foresetの木が１本だけのバージョン
  - Random Foreset

- ハイパーパラメータを調整する。

## Pandas
- なんでも数値にしてくれる　あるじゃん！

## モデルアンサンブルによる予測
- 複数のモデルの評価を合わせて予測モデルをつくるとよい。


# ２日目

2日目は、1日目の復習と、pandasについて勉強し
午後からはkaggleにチャレンジしました。

## Pandas のトピックス
- DataFrame
  - 表
- Series
  - 列

- 行（横を取る）
  ```python
  df.iloc[1,:]
  ```

- 列（縦を取る）
  ```python
  df.ix["Japan"]
  ```

- 平均はmean ミーン
- pandasは、StringをObjectとして表記する。

# データフレーム
- 欠損値を削除

  ```python
  df.dropna
  ```

## pandas の 参考資料
- [10分でパンダスを学ぶ](http://pandas.pydata.org/pandas-docs/stable/10min.html)
- [10分でパンダスを学ぶ　日本語](http://qiita.com/tkazusa/items/23bc0142bf277d397260)
- [チートシート](http://www.analyticsvidhya.com/blog/2015/07/11-steps-perform-data-analysis-pandas-python/)
- [よくつかう文法](http://qiita.com/okadate/items/7b9620a5e64b4e906c42)


## 機械学習コンペティションにおける予測モデリング手法の傾向
[資料](http://yukino.moo.jp/2016-07-06-DDBJ.pdf)

上記の資料がとてもわかりやすいということで、この資料を参考に１日目の講義の復習を行いました。

1. 特徴設計・特徴選択
  - なんのカラムを使うか
2. 予測モデルの学習・チューニング
  - サイキットラーン
3. 予測結果出力

- ロジスティック回帰の回帰は回帰の回帰じゃない。分類を示す。

- 教師あり
  - 分類
      - サポートベクタマシン（SVM）
          - 特徴
            - デープラーニングのまえに流行っていた。
            - すごいきれいにできている。
            - 非線形分類モデルを効率気に行う。
            - 自乗が入った瞬間に非線形
          - 選択するときの判断材料
            - 線形じゃないよね。非線形
            - なんか丸い感じがする。
          - カーネル
            - gaussian kernal  = rvf
      - ランダムフォレスト  (デフォルト１０この木)
          - 特徴
            - 予測モデルは人間が、みてもわかる。
            - 決定きだから
            - 複数の決定機を学習
            - その際、入力例・変数の部分集合をランダムに選択
            - 決定機の予測結果の平均値を最終出力とする
          - 選択するときの判断材料
            - 非線形
            - 枠に張り付いて、分類する。
      - 勾配(コウバイ)ブースティング（グレイデントブースティング）（デフォルト＝100の木）
          - ランダムフォレスト　＋　勾配ブースティング
          - ソフトウィエア：XGBoost
          - 多数のコンペティションで良い成績
          - これがでデファクトスタンダード
      - ディープニューラルネットワーク(DNN)
          - 多数の中間層の導入により複雑なおモデルを表現する
          - TensorFlow(テンサフロー) がシェアNo１
          - Theano(セアノ) PythonでGPUを使うときにつかう。
            - Nビディアがトップになった。
          - Chainer(チェイナー) 日本がつくった。
          - keras(ケラス) セアノを使いやすくしたラッパー
            - 最近日本でも人気がでてきた。
            - tensorFowとTheanoを使える
            - [ニューラルネットワークデモ](http://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=4,2&seed=0.98269&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false)
    - 最近はやっている、ハイパーパラメータの調整　ベイス最適化
      - まだサイキットラーンには実装されてない。
      - ガウシアンプロセス

  - 特徴設計
    １つひつの数値の意味が小さいのは、ディープランニングがとくい、（画像とか）
    魚の長さや重さは、思い意味をもつので、ディープランニングは不得意

    - データを過酷しやすい特徴を作り出す
      - 良い特徴が明示的に与えられているとは限らない
      - Kaggleのトップランカーは、多くの時間がを特徴設計にかけている
    - 様ざなま表現を入力
      - 文章での例：文字数、単語数、カンマの数、etc.
      - 日付での例:年、月、曜日、祝日、etc.
    - 特徴を組み合わせる：
  - 特徴選択
    - 特徴を選択して、もっともよい評価を導く
    - 組み合わせてもOK

## カテゴリの仕方
- One-hot encodding

機械学習するには文字列データはだめ必ず、数値に変換する必要がある。

犬、猫、馬とかすくなかったら、項目を追加して、1,0に変換する方法もある。

isDoc | isCat
:---- | :----
0     | 1
1     | 0

- One-hot encodding のコード
  ```python
  dum = pd.get_dummies(df_train["Sex"])
  ```

- 大量データの場合は、出現率を変換するなど
- is_rareを作ってもいいね。

## matplotlib
- matplotlibは低レベル
- seaborn(シーボーン)、可視化用のライブラリよくあるある分析をできる

## 学んだこと
- ランダムフォレストは、オーバーフィィットしやすい　（基の数、深さを調整する必要あり）
- 効率よく学習するために、K分割交差方は重要。
- モデルに、numpy形式で渡す必要がある。
- モデルに、データフレームを渡すことはできない。
- 欠損値は、平均値や、中央値を設定する。
- 特徴点をを増やすと、ハイパーパラメータの再調整がいる。
。numpy.ndarrayのデータに変換する必要がある。as_matrixでね。)

# 3日目

3日目はグループに分かれて機械学習の応用について、話し合い最後は各チーム毎にスライドにまとめて発表しました。

## 発表内容

- 健康診断のデータから、病気になるかどうかを予測する。
- 出雲大社の付近のお店の営業時間を伸ばす。
- 島根に移住をする人をみつける。
  - 定住したかどうか？のラベルを付ける良いかも

## 各チームの発表を聞いたときの気付き
- 未知のデータとトレーニングデータの特徴点は同じに考えるべき
- Xだけでなく、yをセットでデータを探すのが重要


2泊3日とても大変でしたが、とても楽しい時間が過ごせました。

また、合宿イベントがあれば参加したいです。

ではでは
